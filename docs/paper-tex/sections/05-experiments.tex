% ==============================================================================
% Section 5: 실험 및 평가 (Experiments)
% ==============================================================================

\section{실험 및 평가 (Experiments)}
\label{sec:exp}

% ------------------------------------------------------------------------------
\subsection{Experimental Setup}
\label{sec:exp-setup}

% ==== 5.1.1 Dataset and Scope ====
\subsubsection{Dataset and Scope}
\label{sec:exp-dataset}

본 연구는 엣지 환경에서의 도메인 특화 RAG 시스템 설계를 검증하는 \textbf{파일럿 스터디(pilot study)}이다. 소규모 데이터셋의 통계적 한계를 인지하고, 시스템 아키텍처의 타당성 검증과 엣지 배포 가능성 확인에 초점을 맞춘다.

\paragraph{실험 도메인:}

\begin{table}[h]
\centering
\small
\begin{tabular}{llll}
\toprule
도메인 & 역할 & 데이터 소스 & 비고 \\
\midrule
\textbf{와사비 (Main)} & 주요 검증 & 시즈오카현 가이드라인 PDF, 농업 논문, 위키 & 서론 1.6절 근거 \\
\textbf{SeedBench (Aux)} & 일반화 검증 & 벼 육종 QA 2,264개 & ACL 2025 벤치마크 \\
\bottomrule
\end{tabular}
\caption{실험 도메인 구성}
\label{tab:exp-domains}
\end{table}

\paragraph{QA 데이터셋 생성 (RAGEval 방법론 적용):}

RAGEval \cite{zhu2025rageval}의 시나리오 기반 QA 생성 방법론을 적용하여 평가 데이터셋을 구축하였다:

\begin{enumerate}
\item \textbf{핵심 구절 추출}: 코퍼스에서 도메인 관련 Key Points 자동 추출
\item \textbf{QA 쌍 생성}: LLM을 활용한 질문-답변 쌍 생성
\item \textbf{복잡도 분류}: Basic(단일 사실) / Intermediate(추론) / Advanced(다단계 참조)
\item \textbf{질의 유형 분류} (Know Your RAG \cite{cuconasu2025knowyourrag}):
   \begin{itemize}
   \item Factoid: 단순 사실 확인 질의
   \item Reasoning: 인과관계 추론 필요 질의
   \item Multi-hop: 다중 문서 참조 필요 질의
   \end{itemize}
\end{enumerate}

\paragraph{와사비 데이터셋 구성:}

\begin{table}[h]
\centering
\small
\begin{tabular}{lll}
\toprule
항목 & 규모 & 비고 \\
\midrule
말뭉치 & 402개 청크 & PDF/위키 텍스트 추출 \\
QA 데이터셋 & 220개 쌍 & RAGEval 방법론 적용 \\
카테고리 & 4개 & 재배기술, 환경관리, 병해충, 영양관리 \\
복잡도 & 3단계 & Basic, Intermediate, Advanced \\
질의 유형 & 3종 & Factoid, Reasoning, Multi-hop \\
\bottomrule
\end{tabular}
\caption{와사비 데이터셋 통계}
\label{tab:wasabi-dataset}
\end{table}

\paragraph{통계적 한계:}

본 데이터셋은 BEIR (수천$\sim$수백만 문서)나 LegalBench-RAG (6,889 QA)에 비해 소규모이다. Card et al. \cite{card2020power}의 분석에 따르면, N=220에서 80\% 검정력으로 검출 가능한 최소 효과 크기(MDE)는 약 \textbf{4-5\% MRR 차이}이다. 이보다 작은 개선은 통계적으로 유의하지 않을 수 있다. 비율 지표(Hit Rate, Precision)에는 Wilson score interval을, 연속 지표(MRR, NDCG)에는 표준편차를 함께 보고하여 소표본 불확실성을 명시한다.

% ==== 5.1.2 Baselines ====
\subsubsection{Baselines}
\label{sec:exp-baselines}

다섯 가지 검색 베이스라인을 비교 평가한다:

\begin{table}[h]
\centering
\small
\begin{tabular}{lll}
\toprule
베이스라인 & 설명 & 특징 \\
\midrule
\textbf{Dense-only} & FAISS 임베딩 유사도 검색 & 의미적 유사성 \\
\textbf{BM25} & Sparse 키워드 검색 & 정확한 용어 매칭 \\
\textbf{RRF} & Reciprocal Rank Fusion (Dense+BM25) & 고정 하이브리드 융합 \\
\textbf{Adaptive Hybrid} & Query Specificity 기반 적응형 라우팅 & 쿼리별 최적 방법 선택 \\
\textbf{LightRAG} & Dual-Level 그래프 검색 (Entity + Community) & 지식 그래프 기반 \\
\textbf{PathRAG Hybrid} & 인과관계 그래프 + 하이브리드 검색 & Dense+Sparse+Graph 3채널 융합 \\
\bottomrule
\end{tabular}
\caption{검색 베이스라인 구성}
\label{tab:baselines}
\end{table}

\paragraph{Adaptive Hybrid (제안 - 기초 방법) 특징:}
\begin{itemize}
\item \textbf{Query Specificity Routing}: TF-IDF 기반 쿼리 특성 분석
\item \textbf{도메인 적응}: 전문 용어 쿼리 $\rightarrow$ RRF Hybrid, 의미적 쿼리 $\rightarrow$ Dense-only
\item \textbf{Training-free}: 추가 학습 없이 코퍼스 기반 자동 라우팅
\item \textbf{BEIR 검증}: 외부 벤치마크에서 도메인별 성능 검증 (Section 5.5 참조)
\end{itemize}

\paragraph{LightRAG Baseline \cite{guo2025lightrag}:}
\begin{itemize}
\item \textbf{Entity-Level}: 개별 엔티티 노드 기반 검색
\item \textbf{Community-Level}: Leiden 알고리즘으로 클러스터링된 커뮤니티 요약 활용
\item \textbf{Ego-Network Traversal}: 관련 엔티티의 이웃 노드까지 확장 탐색
\item \textbf{비교 목적}: 본 연구의 HybridDAT 시스템과 성능 대조를 위한 baseline으로 활용
\end{itemize}

\begin{remark}
\textbf{구현 참고}: 베이스라인 수식 및 LightRAG 상세 알고리즘은 Section 3.3 참조. 모든 베이스라인은 공정한 비교를 위해 동일 임베딩 모델(MiniLM-L12-v2)로 자체 구현하였다.
\end{remark}

% ==== 5.1.3 Metrics and K Selection ====
\subsubsection{Metrics and K Selection}
\label{sec:exp-metrics}

\paragraph{검색 성능 메트릭:}
\begin{itemize}
\item Precision@K, Recall@K: 상위 K개 결과의 정밀도/재현율
\item MRR (Mean Reciprocal Rank): 첫 번째 정답 순위의 역수 평균
\item NDCG@K: Normalized Discounted Cumulative Gain
\item Hit Rate@K: 상위 K개 중 최소 1개 정답 포함 여부
\end{itemize}

\paragraph{K=4 선택 근거:}

본 연구에서는 K=4를 주요 평가 기준으로 사용한다:

\begin{enumerate}
\item \textbf{프롬프트 제한}: 8GB RAM에서 최대 4개 문서만 프롬프트에 포함 가능
\item \textbf{응답 시간}: 문서 추가 시 생성 시간 증가, 실시간 응답 위해 제한
\item \textbf{품질 균형}: ``lost in the middle'' 문제 방지 \cite{liu2024lost}
\end{enumerate}

표준 벤치마크 K=1, 5, 10 결과는 Appendix B 참조.

\paragraph{엣지 성능 메트릭:}
\begin{itemize}
\item Cold Start Time, Query Latency (p50/p95/p99), Memory Usage, QPS
\end{itemize}

\paragraph{인과관계 추출 메트릭:}
\begin{itemize}
\item Entity Precision/Recall/F1: 엔티티 추출 정확도
\item Relation Precision/Recall/F1: 관계 추출 정확도
\item 매칭 모드: exact (정확 일치), canonical (ID 기반), fuzzy (유사도 기반)
\end{itemize}

\paragraph{다중 홉 추론 메트릭:}
\begin{itemize}
\item Hop Accuracy: 개별 홉의 정확도
\item Path Exact Match: 전체 경로 일치 여부
\item Supporting Facts P/R/F1: 중간 근거 문서 평가
\item Answer EM/F1: 최종 답변 평가
\end{itemize}

\begin{remark}
\textbf{구현 상세}: 하이퍼파라미터($\theta$=0.85, 작물 보너스 등)는 Section 4.2 참조.
\end{remark}

% ------------------------------------------------------------------------------
\subsection{Results}
\label{sec:exp-results}

\begin{remark}
\textbf{RQ 프레임워크}: 본 섹션의 연구 질문(RQ)은 서론 1.6절의 4가지 연구 목표에 대응한다:
\begin{itemize}
\item RQ1 (목표 1): 경량 LLM 추론 환경 $\rightarrow$ 엣지 성능
\item RQ2 (목표 2): 근거 기반 응답 지원 $\rightarrow$ 검색 및 생성 품질
\item RQ3 (목표 3): 품질-비용 균형 $\rightarrow$ 컴포넌트 기여도 분석
\item RQ4 (목표 4): 성능 지표 평가 $\rightarrow$ 도메인 특화 기능 효과
\end{itemize}
\end{remark}

% ==== 5.2.1 Edge Performance (RQ1) ====
\subsubsection{Edge Performance (RQ1)}
\label{sec:exp-edge}

\paragraph{RQ1:} 제안 시스템이 엣지 환경(8GB RAM)에서 실용적인 성능을 보이는가?

서론 목표 (1): 현장 디바이스에서 동작 가능한 경량 LLM 추론 환경 구축

\begin{table}[h]
\centering
\small
\begin{tabular}{lccl}
\toprule
Metric & Value & Target & Status \\
\midrule
Cold Start Time & \tbd{} s & $<$ 10s & \tbd \\
Index Memory & \tbd{} MB & $<$ 1GB & \tbd \\
Retrieval Latency (p50) & 3,423 ms & $<$ 5s & ✅ \\
Retrieval Latency (p95) & 6,591 ms & $<$ 10s & ✅ \\
Generation Latency (p50) & 2,485 ms & $<$ 5s & ✅ \\
Generation Latency (p95) & 4,310 ms & $<$ 8s & ✅ \\
\textbf{EtE Latency (p50)} & \textbf{6,359 ms} & $<$ 10s & ✅ \\
\textbf{EtE Latency (p95)} & \textbf{10,095 ms} & $<$ 15s & ✅ \\
\textbf{EtE Latency (p99)} & \textbf{10,499 ms} & $<$ 20s & ✅ \\
Throughput (EtE) & 0.16 QPS & $>$ 0.1 & ✅ \\
\bottomrule
\end{tabular}
\caption{Edge Performance Metrics}
\label{tab:edge-performance}
\end{table}

\textit{CPU 환경(Qwen3-Embedding-0.6B, Qwen3-0.6B) 기준. GPU 환경에서 2-5x 성능 향상 예상.}

\paragraph{Memory Scaling:}

문서 수 증가에 따른 메모리 사용량은 선형적으로 증가하며, 400개 문서 기준 약 \tbd{} KB/doc의 메모리 효율을 보인다.

% ==== 5.2.2 Retrieval Quality (RQ2) ====
\subsubsection{Retrieval Quality (RQ2)}
\label{sec:exp-retrieval}

\paragraph{RQ2:} 제안 시스템이 근거 기반 응답을 효과적으로 지원하는가?

서론 목표 (2): 매뉴얼·가이드 등을 참조 지식으로 정리하여 근거 기반 응답 지원

본 섹션에서는 검색 성능(IR 메트릭)과 생성 품질(RAGAS 메트릭)을 통합 평가하여, 시스템이 근거 기반 응답을 얼마나 효과적으로 지원하는지 검증한다.

% ---- 5.2.2.1 Baseline Comparison ----
\paragraph{5.2.2.1 Baseline Comparison}

\begin{table}[h]
\centering
\small
\begin{tabular}{lccccc}
\toprule
Method & P@4 & R@4 & MRR & NDCG@4 & Hit@4 \\
\midrule
Dense-only & \tbd & \tbd & \tbd & \tbd & \tbd \\
BM25 & \tbd & \tbd & \tbd & \tbd & \tbd \\
RRF & \tbd & \tbd & \tbd & \tbd & \tbd \\
\textbf{Adaptive Hybrid} & \textbf{\tbd} & \textbf{\tbd} & \textbf{\tbd} & \textbf{\tbd} & \textbf{\tbd} \\
\textbf{LightRAG} & \textbf{\tbd} & \textbf{\tbd} & \textbf{\tbd} & \textbf{\tbd} & \textbf{\tbd} \\
\textbf{PathRAG Hybrid} & \textbf{\tbd} & \textbf{\tbd} & \textbf{\tbd} & \textbf{\tbd} & \textbf{\tbd} \\
\bottomrule
\end{tabular}
\caption{Baseline Performance Comparison (N=220)}
\label{tab:baseline}
\end{table}

\textit{각 값은 mean $\pm$ std 형식. MDE $\approx$ 4-5\%이므로 이보다 작은 차이는 통계적으로 유의하지 않을 수 있음.}

\paragraph{분석:}

[실험 결과 생성 후 작성]

\begin{itemize}
\item Dense vs BM25: 의미적 vs 키워드 매칭 특성 비교
\item RRF 한계: 단순 랭킹 융합으로 관계 정보 미활용
\item LightRAG baseline: 그래프 기반 엔티티/커뮤니티 검색 성능 벤치마크
\item HybridDAT 효과: 도메인 특화 온톨로지와 3채널 융합으로 성능 개선
\end{itemize}

% ---- 5.2.2.2 RAG Quality Evaluation (RAGAS) ----
\paragraph{5.2.2.2 RAG Quality Evaluation (RAGAS)}

전통적인 IR 메트릭(MRR, NDCG 등)은 검색 품질만 측정하며, 최종 답변의 품질은 평가하지 못한다. 본 연구에서는 RAGAS \cite{es2024ragas} 프레임워크를 활용하여 \textbf{Reference-free} 방식으로 생성 품질을 평가한다.

\begin{table}[h]
\centering
\small
\begin{tabular}{lcccc}
\toprule
Method & Faithfulness & Answer Relevancy & Context Precision & Context Recall \\
\midrule
Dense-only & \tbd & \tbd & \tbd & \tbd \\
BM25 & \tbd & \tbd & \tbd & \tbd \\
RRF & \tbd & \tbd & \tbd & \tbd \\
\textbf{LightRAG} & \textbf{\tbd} & \textbf{\tbd} & \textbf{\tbd} & \textbf{\tbd} \\
\bottomrule
\end{tabular}
\caption{RAGAS Evaluation Results (N=220)}
\label{tab:ragas}
\end{table}

\textit{평가 LLM: Qwen3-0.6B (로컬), 임베딩: MiniLM-L12-v2. 각 메트릭은 0-1 범위, 높을수록 좋음.}

\paragraph{RAGAS 메트릭 설명:}
\begin{itemize}
\item \textbf{Faithfulness}: 답변이 검색된 context에 근거하는가 (환각 억제 정도)
\item \textbf{Answer Relevancy}: 답변이 질문에 적절히 대응하는가
\item \textbf{Context Precision}: 검색된 문서들이 답변 생성에 유용한가
\item \textbf{Context Recall}: 정답 생성에 필요한 정보가 context에 포함되었는가
\end{itemize}

\paragraph{분석:}

[실험 결과 생성 후 작성]

\begin{itemize}
\item \textbf{Faithfulness}: HybridDAT의 온톨로지 매칭이 정확한 근거 문서 제공 $\rightarrow$ 환각 감소 기대
\item \textbf{Context Precision}: PathRAG 인과관계 경로 탐색으로 관련 정보 집중 $\rightarrow$ 정밀도 향상 기대
\item \textbf{Answer Relevancy}: DAT 질의 적응형 가중치로 질문 의도에 맞는 검색 균형 $\rightarrow$ 적합성 향상 기대
\end{itemize}

\begin{remark}
\textbf{실행 방법}: \code{python -m benchmarking.experiments.ragas\_eval --qa-file QA\_PATH --output OUTPUT\_PATH}
\end{remark}

% ==== 5.2.3 Ablation Study (RQ3) ====
\subsubsection{Ablation Study (RQ3)}
\label{sec:exp-ablation}

\paragraph{RQ3:} HybridDAT의 각 컴포넌트가 품질-비용 균형에 얼마나 기여하는가?

서론 목표 (3): 질의 유형에 따라 검색 및 컨텍스트 조절로 품질-비용 균형

\begin{table}[h]
\centering
\small
\begin{tabular}{lcccccc}
\toprule
Configuration & RRF & DAT & Ontology & PathRAG & MRR & $\Delta$MRR \\
\midrule
Dense-only (Base) & - & - & - & - & \tbd & -- \\
+RRF & ✓ & - & - & - & \tbd & +\tbd \\
+DAT & - & ✓ & - & - & \tbd & +\tbd \\
+RRF+DAT & ✓ & ✓ & - & - & \tbd & +\tbd \\
+Ontology & ✓ & ✓ & ✓ & - & \tbd & +\tbd \\
\textbf{HybridDAT (Full)} & ✓ & ✓ & ✓ & ✓ & \textbf{\tbd} & \textbf{+\tbd} \\
\bottomrule
\end{tabular}
\caption{Ablation Results (N=220)}
\label{tab:ablation}
\end{table}

\textit{$\Delta$는 Base 대비 누적 개선. Crop Filter와 Dedup은 성능 저하로 제외됨.}

\paragraph{Key Findings:}

\begin{enumerate}
\item \textbf{RRF (Reciprocal Rank Fusion)}: Dense + Sparse 결과를 랭킹 기반 융합하여 안정적 성능
\item \textbf{DAT (Dynamic Alpha Tuning)}: 질의 특성에 따른 Dense/Sparse 가중치 동적 조정
\item \textbf{Ontology Matching}: 농업 온톨로지 개념 매칭으로 도메인 관련성 부스팅
\item \textbf{PathRAG}: 인과관계 그래프 기반 경로 탐색으로 Multi-hop 질의 대응
\end{enumerate}

\paragraph{Deprecated Components (성능 저하로 제외):}
\begin{itemize}
\item \st{Crop Filter}: 작물명 기반 필터링 - 오탐/미탐으로 인한 정확도 저하
\item \st{Semantic Dedup}: 임베딩 유사도 중복 제거 - 과도한 다양성 손실
\end{itemize}

\paragraph{질의 유형별 컴포넌트 효과:}

\begin{table}[h]
\centering
\small
\begin{tabular}{lll}
\toprule
질의 유형 & 효과적 컴포넌트 & 이유 \\
\midrule
Factoid & RRF + Ontology & 정확한 용어 매칭 \\
Reasoning & DAT & 의미적/키워드 검색 균형 조정 \\
Multi-hop & PathRAG & 인과관계 경로 탐색 \\
\bottomrule
\end{tabular}
\caption{질의 유형별 컴포넌트 기여도}
\label{tab:ablation-querytype}
\end{table}

% ---- 5.2.3.1 그래프 빌드 모드 비교 ----
\paragraph{5.2.3.1 그래프 빌드 모드 비교}

인과관계 그래프 구축 방식에 따른 성능 차이를 분석한다.

\begin{table}[h]
\centering
\small
\begin{tabular}{lcccccc}
\toprule
빌드 모드 & 설명 & Entity P & Entity R & Entity F1 & Relation F1 & MRR \\
\midrule
rule\_only & 규칙 기반 패턴 매칭 & \tbd & \tbd & \tbd & \tbd & \tbd \\
llm\_only & LLM 기반 추출 & \tbd & \tbd & \tbd & \tbd & \tbd \\
\textbf{hybrid} & 규칙 + LLM 결합 & \textbf{\tbd} & \textbf{\tbd} & \textbf{\tbd} & \textbf{\tbd} & \textbf{\tbd} \\
\bottomrule
\end{tabular}
\caption{Graph Build Mode Comparison}
\label{tab:graph-mode}
\end{table}

\textit{Entity/Relation 메트릭은 \code{causal\_extraction\_gold.jsonl} 기준 평가.}

\paragraph{분석:}
\begin{itemize}
\item \textbf{rule\_only}: 저비용, 빠른 실행, 패턴 기반 추출로 일관성 높음
\item \textbf{llm\_only}: 높은 추출 품질, 추론 비용 증가, 암묵적 관계 포착 가능
\item \textbf{hybrid}: 규칙으로 명시적 관계 추출 후 LLM으로 보완 $\rightarrow$ 품질과 비용 균형
\end{itemize}

\begin{remark}
\textbf{실행 방법}: \code{python -m benchmarking.experiments.llm\_graph\_ab\_test --corpus CORPUS\_PATH --output OUTPUT\_PATH}
\end{remark}

% ==== 5.2.4 Domain Analysis (RQ4) ====
\subsubsection{Domain Analysis (RQ4)}
\label{sec:exp-domain}

\paragraph{RQ4:} 도메인 특화 기능들이 농업 도메인 질의에 효과적인가?

서론 목표 (4): 응답 시간, 메모리, 정확도 등 지표를 통한 성능 평가

\begin{table}[h]
\centering
\small
\begin{tabular}{lcccc}
\toprule
분석 기준 & 구분 & N & MRR & NDCG@4 \\
\midrule
\multirow{4}{*}{\textbf{카테고리}} & 재배기술 & \tbd & \tbd & \tbd \\
 & 환경관리 & \tbd & \tbd & \tbd \\
 & 병해충 & \tbd & \tbd & \tbd \\
 & 영양관리 & \tbd & \tbd & \tbd \\
\midrule
\multirow{3}{*}{\textbf{복잡도}} & Basic & \tbd & \tbd & \tbd \\
 & Intermediate & \tbd & \tbd & \tbd \\
 & Advanced & \tbd & \tbd & \tbd \\
\bottomrule
\end{tabular}
\caption{Performance by Category and Complexity}
\label{tab:domain-analysis}
\end{table}

\paragraph{온톨로지 효과:}
\begin{itemize}
\item With ontology matching: MRR = \tbd{} (N = \tbd)
\item Without ontology matching: MRR = \tbd{} (N = \tbd)
\item Improvement: \tbd\%
\end{itemize}

% ---- 5.2.4.1 다중 홉 추론 평가 ----
\paragraph{5.2.4.1 다중 홉 추론 평가}

Multi-hop 질의에 대한 추론 경로 정확도를 평가한다.

\begin{table}[h]
\centering
\small
\begin{tabular}{lcc}
\toprule
메트릭 & 설명 & 값 \\
\midrule
Hop Accuracy & 개별 홉 정확도 & \tbd \\
Path Exact Match & 전체 경로 일치 & \tbd \\
Supporting Facts P & 중간 근거 정밀도 & \tbd \\
Supporting Facts R & 중간 근거 재현율 & \tbd \\
Supporting Facts F1 & 중간 근거 F1 & \tbd \\
Answer EM & 최종 답변 정확 일치 & \tbd \\
Answer F1 & 최종 답변 F1 & \tbd \\
\bottomrule
\end{tabular}
\caption{Multi-hop Reasoning Evaluation}
\label{tab:multihop}
\end{table}

\textit{평가 데이터: \code{multihop\_gold.jsonl} (N = \tbd)}

\paragraph{메트릭 설명:}
\begin{itemize}
\item \textbf{Hop Accuracy}: 각 추론 단계(홉)가 정답 경로와 일치하는 비율
\item \textbf{Path Exact Match}: 전체 추론 경로가 정답과 완전히 일치하는 비율
\item \textbf{Supporting Facts}: 답변 도출에 사용된 중간 근거 문서의 정확도
\end{itemize}

\begin{remark}
\textbf{실행 방법}: \code{python -m benchmarking.experiments.multihop\_eval --gold benchmarking/data/multihop\_gold.jsonl --output OUTPUT\_PATH}
\end{remark}

% ---- 5.2.4.2 인과관계 추출 품질 평가 ----
\paragraph{5.2.4.2 인과관계 추출 품질 평가}

CausalExtractor의 엔티티/관계 추출 정확도를 평가한다.

\begin{table}[h]
\centering
\small
\begin{tabular}{lcccccc}
\toprule
매칭 모드 & Entity P & Entity R & Entity F1 & Relation P & Relation R & Relation F1 \\
\midrule
exact & \tbd & \tbd & \tbd & \tbd & \tbd & \tbd \\
canonical & \tbd & \tbd & \tbd & \tbd & \tbd & \tbd \\
fuzzy & \tbd & \tbd & \tbd & \tbd & \tbd & \tbd \\
\bottomrule
\end{tabular}
\caption{Causal Extraction Quality}
\label{tab:causal-extraction}
\end{table}

\textit{평가 데이터: \code{causal\_extraction\_gold.jsonl} (N = \tbd)}

\paragraph{매칭 모드 설명:}
\begin{itemize}
\item \textbf{exact}: 정규화된 텍스트 정확 일치
\item \textbf{canonical}: canonical\_id 기반 매칭 (동의어 허용)
\item \textbf{fuzzy}: 유사도 기반 매칭 ($\theta \geq 0.85$)
\end{itemize}

\paragraph{엔티티 타입별 분석:}

\begin{table}[h]
\centering
\small
\begin{tabular}{lcc}
\toprule
엔티티 타입 & 설명 & F1 \\
\midrule
crop & 작물명 & \tbd \\
disease & 병해충 & \tbd \\
environment & 환경 요인 & \tbd \\
practice & 재배 관리 & \tbd \\
nutrient & 영양/양액 & \tbd \\
stage & 생육 단계 & \tbd \\
\bottomrule
\end{tabular}
\caption{엔티티 타입별 추출 성능}
\label{tab:entity-types}
\end{table}

\begin{remark}
\textbf{실행 방법}: \code{python -m benchmarking.experiments.causal\_extraction\_eval --gold benchmarking/data/causal\_extraction\_gold.jsonl --mode hybrid --output OUTPUT\_PATH}
\end{remark}

% ------------------------------------------------------------------------------
\subsection{External Benchmark Validation (BEIR)}
\label{sec:exp-beir}

외부 벤치마크를 통한 일반화 검증은 별도 문서에서 상세히 다룬다:

\textbf{[05\_experiments\_beir.md]} - BEIR를 통한 도메인별 Adaptive Hybrid 검증

\paragraph{핵심 결과:}

\begin{itemize}
\item \textbf{Terminology-Heavy 도메인} (SciFact, NFCorpus, TREC-COVID): Adaptive Hybrid이 RRF Hybrid와 동등하거나 우수
  \begin{itemize}
  \item SciFact: Adaptive Hybrid 0.764 NDCG@10 (+1.7\% vs RRF)
  \item NFCorpus: Adaptive Hybrid 0.358 NDCG@10 (+5.0\% vs RRF)
  \end{itemize}

\item \textbf{Semantics-Dominant 도메인} (ArguAna, CQADupstack): Dense-only로 정확히 라우팅되어 성능 향상
  \begin{itemize}
  \item ArguAna: Adaptive Hybrid 0.498 (+2.3\% vs RRF, Dense와 동등)
  \item CQADupstack: Adaptive Hybrid 0.327 NDCG@10 (+5.5\% vs RRF)
  \end{itemize}

\item \textbf{평균 성능}: Adaptive Hybrid 0.457 (RRF 고정 0.430 대비 +6.3\% 개선)

\item \textbf{통계적 유의성}: Paired t-test $p < 0.01$ 수준에서 유의미
\end{itemize}

자세한 도메인 분류 체계, BM25 노이즈 분석, Query Specificity 계산, 통계적 검증은 해당 문서 참조.

% ------------------------------------------------------------------------------
\subsection{Discussion}
\label{sec:exp-discussion}

% ==== 5.3.1 Key Findings ====
\subsubsection{Key Findings}
\label{sec:exp-findings}

\begin{enumerate}
\item \textbf{RQ1 (엣지 환경)}: 8GB RAM 제약 하에서 실시간 응답 가능 - EtE p95 $<$ 15s, 0.16 QPS 달성
\item \textbf{RQ2 (근거 기반 응답)}: Dense+Sparse+PathRAG 3채널 융합으로 검색 품질 및 생성 품질 개선
\item \textbf{RQ3 (품질-비용 균형)}: DAT 동적 가중치와 온톨로지 매칭으로 질의 유형별 최적 검색 전략
\item \textbf{RQ4 (도메인 특화)}: 농업 온톨로지 연계로 환경/병해충 관련 질의에서 효과적
\end{enumerate}

% ==== 5.3.2 Limitations ====
\subsubsection{Limitations}
\label{sec:exp-limitations}

본 연구는 다음 한계를 명시적으로 인정한다:

\begin{table}[h]
\centering
\small
\begin{tabular}{llp{6cm}}
\toprule
한계 & 설명 & 영향 \\
\midrule
\textbf{L1. 소규모 데이터셋} & N=220, MDE $\sim$4-5\% & 미세 차이 검출 불가, 통계적 검정력 제한 \\
\textbf{L2. 단일 도메인} & 와사비 단일 작물 특화 & 다른 작물/도메인으로 일반화 검증 필요 \\
\textbf{L3. 합성 평가 데이터} & LLM 생성 QA, 전문가 검증 없음 & 실제 농가 질의 패턴과 차이 가능 \\
\bottomrule
\end{tabular}
\caption{연구 한계 요약}
\label{tab:limitations}
\end{table}

베이스라인 자체 구현 한계 및 향후 연구 방향은 Section 6.2 참조.

% ==== 5.3.3 Threats to Validity ====
\subsubsection{Threats to Validity}
\label{sec:exp-threats}

\begin{table}[h]
\centering
\small
\begin{tabular}{llp{5cm}}
\toprule
유형 & 위협 & 완화 조치 \\
\midrule
Internal & 베이스라인 구현 편향 & 동일 인프라/모델 사용, 코드 공개 \\
External & 단일 도메인 & ``pilot study''로 범위 명시 \\
Construct & K=4 비표준 & 근거 명시, Appendix B에 K=1,5,10 제공 \\
Statistical & 소표본 & MDE 명시, 비율 지표에 Wilson CI, 연속 지표에 std 보고 \\
\bottomrule
\end{tabular}
\caption{타당성 위협과 완화 조치}
\label{tab:threats}
\end{table}

% ==============================================================================
% APPENDICES
% ==============================================================================

\appendix

\section{Reproducibility}
\label{app:reproducibility}

\subsection{실험 재현 방법}
\label{app:repro-method}

\begin{verbatim}
cd era-smartfarm-rag

# 1. 전체 실험 실행
python -m benchmarking.experiments.run_all_experiments \
    --corpus ../dataset-pipeline/output/wasabi_en_ko_parallel.jsonl \
    --qa-file ../dataset-pipeline/output/wasabi_qa_dataset.jsonl \
    --output-dir output/experiments

# 2. 논문용 결과 생성
python -m benchmarking.reporters.PaperResultsReporter \
    --experiments-dir output/experiments \
    --output-dir output/paper

# 3. 그래프 빌드 모드 비교
python -m benchmarking.experiments.llm_graph_ab_test \
    --corpus ../dataset-pipeline/output/wasabi_en_ko_parallel.jsonl \
    --output-dir output/experiments/graph_ab

# 4. 인과관계 추출 평가
python -m benchmarking.experiments.causal_extraction_eval \
    --gold benchmarking/data/causal_extraction_gold.jsonl \
    --mode hybrid \
    --output-dir output/experiments/causal

# 5. 다중 홉 추론 평가
python -m benchmarking.experiments.multihop_eval \
    --gold benchmarking/data/multihop_gold.jsonl \
    --output-dir output/experiments/multihop
\end{verbatim}

\subsection{출력 파일}
\label{app:repro-outputs}

\begin{verbatim}
output/
├── experiments/
│   ├── baselines/baseline_summary.json
│   ├── ablation/ablation_summary.json
│   ├── edge/edge_benchmark_summary.json
│   ├── domain/domain_analysis_summary.json
│   ├── graph_ab/graph_mode_comparison.json
│   ├── causal/extraction_metrics.json
│   └── multihop/multihop_metrics.json
└── paper/
    ├── table1_baseline.tex
    ├── table2_ablation.tex
    └── figure_data.json
\end{verbatim}

\section{Additional K Values}
\label{app:additional-k}

표준 벤치마크와의 비교를 위한 추가 K 값 결과.

\begin{table}[h]
\centering
\small
\begin{tabular}{lccccc}
\toprule
Method & P@1 & P@5 & P@10 & R@5 & R@10 \\
\midrule
Dense-only & \tbd & \tbd & \tbd & \tbd & \tbd \\
BM25 & \tbd & \tbd & \tbd & \tbd & \tbd \\
RRF & \tbd & \tbd & \tbd & \tbd & \tbd \\
\textbf{LightRAG} & \textbf{\tbd} & \textbf{\tbd} & \textbf{\tbd} & \textbf{\tbd} & \textbf{\tbd} \\
\bottomrule
\end{tabular}
\caption{Precision/Recall at Various K}
\label{tab:precision-recall-k}
\end{table}

\begin{table}[h]
\centering
\small
\begin{tabular}{lccc}
\toprule
Method & NDCG@1 & NDCG@5 & NDCG@10 \\
\midrule
Dense-only & \tbd & \tbd & \tbd \\
BM25 & \tbd & \tbd & \tbd \\
RRF & \tbd & \tbd & \tbd \\
\textbf{LightRAG} & \textbf{\tbd} & \textbf{\tbd} & \textbf{\tbd} \\
\bottomrule
\end{tabular}
\caption{NDCG at Various K}
\label{tab:ndcg-k}
\end{table}
